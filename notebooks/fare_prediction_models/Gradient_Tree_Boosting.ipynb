{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_pickle(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>total_custom</th>\n",
       "      <th>weekend_Trip</th>\n",
       "      <th>hour</th>\n",
       "      <th>airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-31 23:55:09</td>\n",
       "      <td>2017-12-31 23:59:14</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-31 23:30:36</td>\n",
       "      <td>2017-12-31 23:37:20</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>7.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.733333</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>7.80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-31 23:02:26</td>\n",
       "      <td>2017-12-31 23:19:37</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.183333</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-31 23:12:24</td>\n",
       "      <td>2017-12-31 23:16:55</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.516667</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-31 23:53:11</td>\n",
       "      <td>2017-12-31 23:55:40</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  RatecodeID  PULocationID  \\\n",
       "0  2017-12-31 23:55:09   2017-12-31 23:59:14           1            17   \n",
       "1  2017-12-31 23:30:36   2017-12-31 23:37:20           1            61   \n",
       "2  2017-12-31 23:02:26   2017-12-31 23:19:37           1           228   \n",
       "3  2017-12-31 23:12:24   2017-12-31 23:16:55           1           228   \n",
       "4  2017-12-31 23:53:11   2017-12-31 23:55:40           1           225   \n",
       "\n",
       "   DOLocationID  passenger_count  trip_distance  fare_amount  extra  mta_tax  \\\n",
       "0            61                1            1.1          5.5    0.5      0.5   \n",
       "1            49                1            1.1          6.5    0.5      0.5   \n",
       "2           188                1            4.1         15.5    0.5      0.5   \n",
       "3            26                1            0.8          5.0    0.5      0.5   \n",
       "4           225                1            0.5          4.0    0.5      0.5   \n",
       "\n",
       "    ...     total_amount  payment_type  trip_type  trip_duration  month  day  \\\n",
       "0   ...             6.80             2        1.0       4.083333     12    6   \n",
       "1   ...             7.80             2        1.0       6.733333     12    6   \n",
       "2   ...            20.15             1        1.0      17.183333     12    6   \n",
       "3   ...             6.30             2        1.0       4.516667     12    6   \n",
       "4   ...             5.30             2        1.0       2.483333     12    6   \n",
       "\n",
       "   total_custom  weekend_Trip  hour  airport  \n",
       "0          6.80             1    23        0  \n",
       "1          7.80             1    23        0  \n",
       "2         20.15             1    23        0  \n",
       "3          6.30             1    23        0  \n",
       "4          5.30             1    23        0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframes for model testing. Models cannot parse datetime columns, so have to drop them\n",
    "\n",
    "df_1 = df_new[['month','day','hour','PULocationID','DOLocationID','fare_amount']]\n",
    "df_2 = df_new[['month','day','hour','PULocationID','DOLocationID','fare_amount','airport']]\n",
    "df_3 = df_new[['month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_distance']]\n",
    "df_4 = df_new[['month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_duration']]\n",
    "df_5 = df_new[['month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_distance','trip_duration']]\n",
    "df_6 = df_new[['trip_distance','trip_duration','fare_amount']]\n",
    "df_7 = df_new[['day','hour','PULocationID','DOLocationID','fare_amount']]\n",
    "df_8 = df_new[['day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','fare_amount']]\n",
    "df_9 = df_new[['day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','RatecodeID','fare_amount']]\n",
    "df_10 = df_new[['day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','RatecodeID','fare_amount','passenger_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_1 = df_1.drop(['fare_amount'], axis = 1)\n",
    "labels_1 = df_1[['fare_amount']]\n",
    "\n",
    "features_2 = df_2.drop(['fare_amount'], axis = 1)\n",
    "labels_2 = df_2[['fare_amount']]\n",
    "\n",
    "features_3 = df_3.drop(['fare_amount'], axis = 1)\n",
    "labels_3 = df_3[['fare_amount']]\n",
    "\n",
    "features_4 = df_4.drop(['fare_amount'], axis = 1)\n",
    "labels_4 = df_4[['fare_amount']]\n",
    "\n",
    "features_5 = df_5.drop(['fare_amount'], axis = 1)\n",
    "labels_5 = df_5[['fare_amount']]\n",
    "\n",
    "features_6 = df_6.drop(['fare_amount'], axis = 1)\n",
    "labels_6 = df_6[['fare_amount']]\n",
    "\n",
    "features_7 = df_7.drop(['fare_amount'], axis = 1)\n",
    "labels_7 = df_7[['fare_amount']]\n",
    "\n",
    "features_8 = df_8.drop(['fare_amount'], axis = 1)\n",
    "labels_8 = df_8[['fare_amount']]\n",
    "\n",
    "features_9 = df_9.drop(['fare_amount'], axis = 1)\n",
    "labels_9 = df_9[['fare_amount']]\n",
    "\n",
    "features_10 = df_10.drop(['fare_amount'], axis = 1)\n",
    "labels_10 = df_10[['fare_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "train_features_1, test_features_1, train_labels_1, test_labels_1 = train_test_split(features_1, labels_1, test_size = 0.10, random_state = 42)\n",
    "train_features_2, test_features_2, train_labels_2, test_labels_2 = train_test_split(features_2, labels_2, test_size = 0.10, random_state = 42)\n",
    "train_features_3, test_features_3, train_labels_3, test_labels_3 = train_test_split(features_3, labels_3, test_size = 0.10, random_state = 42)\n",
    "train_features_4, test_features_4, train_labels_4, test_labels_4 = train_test_split(features_4, labels_4, test_size = 0.10, random_state = 42)\n",
    "train_features_5, test_features_5, train_labels_5, test_labels_5 = train_test_split(features_5, labels_5, test_size = 0.10, random_state = 42)\n",
    "train_features_6, test_features_6, train_labels_6, test_labels_6 = train_test_split(features_6, labels_6, test_size = 0.10, random_state = 42)\n",
    "train_features_7, test_features_7, train_labels_7, test_labels_7 = train_test_split(features_7, labels_7, test_size = 0.10, random_state = 42)\n",
    "train_features_8, test_features_8, train_labels_8, test_labels_8 = train_test_split(features_8, labels_8, test_size = 0.10, random_state = 42)\n",
    "train_features_9, test_features_9, train_labels_9, test_labels_9 = train_test_split(features_9, labels_9, test_size = 0.10, random_state = 42)\n",
    "train_features_10, test_features_10, train_labels_10, test_labels_10 = train_test_split(features_10, labels_10, test_size = 0.10, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_features_1.mean(axis=0)\n",
    "std = train_features_1.std(axis=0)\n",
    "train_features_1 = (train_features_1 - mean) / std\n",
    "test_features_1 = (test_features_1 - mean) / std\n",
    "\n",
    "mean = train_features_2.mean(axis=0)\n",
    "std = train_features_2.std(axis=0)\n",
    "train_features_2 = (train_features_2 - mean) / std\n",
    "test_features_2 = (test_features_2 - mean) / std\n",
    "\n",
    "mean = train_features_3.mean(axis=0)\n",
    "std = train_features_3.std(axis=0)\n",
    "train_features_3 = (train_features_3 - mean) / std\n",
    "test_features_3 = (test_features_3 - mean) / std\n",
    "\n",
    "mean = train_features_4.mean(axis=0)\n",
    "std = train_features_4.std(axis=0)\n",
    "train_features_4 = (train_features_4 - mean) / std\n",
    "test_features_4 = (test_features_4 - mean) / std\n",
    "\n",
    "mean = train_features_5.mean(axis=0)\n",
    "std = train_features_5.std(axis=0)\n",
    "train_features_5 = (train_features_5 - mean) / std\n",
    "test_features_5 = (test_features_5 - mean) / std\n",
    "\n",
    "mean = train_features_6.mean(axis=0)\n",
    "std = train_features_6.std(axis=0)\n",
    "train_features_6 = (train_features_6 - mean) / std\n",
    "test_features_6 = (test_features_6 - mean) / std\n",
    "\n",
    "mean = train_features_7.mean(axis=0)\n",
    "std = train_features_7.std(axis=0)\n",
    "train_features_7 = (train_features_7 - mean) / std\n",
    "test_features_7 = (test_features_7 - mean) / std\n",
    "\n",
    "mean = train_features_8.mean(axis=0)\n",
    "std = train_features_8.std(axis=0)\n",
    "train_features_8 = (train_features_8 - mean) / std\n",
    "test_features_8 = (test_features_8 - mean) / std\n",
    "\n",
    "mean = train_features_9.mean(axis=0)\n",
    "std = train_features_9.std(axis=0)\n",
    "train_features_9 = (train_features_9 - mean) / std\n",
    "test_features_9 = (test_features_9 - mean) / std\n",
    "\n",
    "mean = train_features_10.mean(axis=0)\n",
    "std = train_features_10.std(axis=0)\n",
    "train_features_10 = (train_features_10 - mean) / std\n",
    "test_features_10 = (test_features_10 - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (2024739, 5)\n",
      "Training Labels Shape: (2024739, 1)\n",
      "Testing Features Shape: (224972, 5)\n",
      "Testing Labels Shape: (224972, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features_1.shape)\n",
    "print('Training Labels Shape:', train_labels_1.shape)\n",
    "print('Testing Features Shape:', test_features_1.shape)\n",
    "print('Testing Labels Shape:', test_labels_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gradient Boosting for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rf1 = GradientBoostingRegressor()\n",
    "rf2 = GradientBoostingRegressor()\n",
    "rf3 = GradientBoostingRegressor()\n",
    "rf4 = GradientBoostingRegressor()\n",
    "rf5 = GradientBoostingRegressor()\n",
    "rf6 = GradientBoostingRegressor()\n",
    "rf7 = GradientBoostingRegressor()\n",
    "rf8 = GradientBoostingRegressor()\n",
    "rf9 = GradientBoostingRegressor()\n",
    "rf10 = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "rf1.fit(train_features_1, train_labels_1.values.ravel())\n",
    "rf2.fit(train_features_2, train_labels_2.values.ravel())\n",
    "rf3.fit(train_features_3, train_labels_3.values.ravel())\n",
    "rf4.fit(train_features_4, train_labels_4.values.ravel())\n",
    "rf5.fit(train_features_5, train_labels_5.values.ravel())\n",
    "rf6.fit(train_features_6, train_labels_6.values.ravel())\n",
    "rf7.fit(train_features_7, train_labels_7.values.ravel())\n",
    "rf8.fit(train_features_8, train_labels_8.values.ravel())\n",
    "rf9.fit(train_features_9, train_labels_9.values.ravel())\n",
    "rf10.fit(train_features_10, train_labels_10.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the models to pickle file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf1.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf1, fid)\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf2.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf2, fid)    \n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf3.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf3, fid)    \n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf4.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf4, fid)    \n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf5.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf5, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the models from pickle file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf1.pkl', 'rb') as fid:\n",
    "    rf1 = pickle.load(fid)\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf2.pkl', 'rb') as fid:\n",
    "    rf2 = pickle.load(fid)\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf3.pkl', 'rb') as fid:\n",
    "    rf3 = pickle.load(fid)\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf4.pkl', 'rb') as fid:\n",
    "    rf4 = pickle.load(fid)\n",
    "with open(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\rf5.pkl', 'rb') as fid:\n",
    "    rf5 = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GradientBoostingRegressor on df_1 training set: 0.30\n",
      "Accuracy of GradientBoostingRegressor on df_1 test set: 0.30\n",
      "\n",
      "Accuracy of GradientBoostingRegressor on df_2 training set: 0.31\n",
      "Accuracy of GradientBoostingRegressor on df_2 test set: 0.32\n",
      "\n",
      "Accuracy of GradientBoostingRegressor on df_3 training set: 0.89\n",
      "Accuracy of GradientBoostingRegressor on df_3 test set: 0.89\n",
      "\n",
      "Accuracy of GradientBoostingRegressor on df_4 training set: 0.86\n",
      "Accuracy of GradientBoostingRegressor on df_4 test set: 0.86\n",
      "\n",
      "Accuracy of GradientBoostingRegressor on df_5 training set: 0.95\n",
      "Accuracy of GradientBoostingRegressor on df_5 test set: 0.95\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of GradientBoostingRegressor on df_1 training set: {:.2f}'\n",
    "     .format(rf1.score(train_features_1, train_labels_1)))\n",
    "print('Accuracy of GradientBoostingRegressor on df_1 test set: {:.2f}'\n",
    "     .format(rf1.score(test_features_1, test_labels_1)))\n",
    "print()\n",
    "print('Accuracy of GradientBoostingRegressor on df_2 training set: {:.2f}'\n",
    "     .format(rf2.score(train_features_2, train_labels_2)))\n",
    "print('Accuracy of GradientBoostingRegressor on df_2 test set: {:.2f}'\n",
    "     .format(rf2.score(test_features_2, test_labels_2)))\n",
    "print()\n",
    "print('Accuracy of GradientBoostingRegressor on df_3 training set: {:.2f}'\n",
    "     .format(rf3.score(train_features_3, train_labels_3)))\n",
    "print('Accuracy of GradientBoostingRegressor on df_3 test set: {:.2f}'\n",
    "     .format(rf3.score(test_features_3, test_labels_3)))\n",
    "print()\n",
    "print('Accuracy of GradientBoostingRegressor on df_4 training set: {:.2f}'\n",
    "     .format(rf4.score(train_features_4, train_labels_4)))\n",
    "print('Accuracy of GradientBoostingRegressor on df_4 test set: {:.2f}'\n",
    "     .format(rf4.score(test_features_4, test_labels_4)))\n",
    "print()\n",
    "print('Accuracy of GradientBoostingRegressor on df_5 training set: {:.2f}'\n",
    "     .format(rf5.score(train_features_5, train_labels_5)))\n",
    "print('Accuracy of GradientBoostingRegressor on df_5 test set: {:.2f}'\n",
    "     .format(rf5.score(test_features_5, test_labels_5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting from Input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_1.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_1.csv',mode = 'w', index=False)\n",
    "df_2.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_2.csv',mode = 'w', index=False)\n",
    "df_3.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_3.csv',mode = 'w', index=False)\n",
    "df_4.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_4.csv',mode = 'w', index=False)\n",
    "df_5.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\df_5.csv',mode = 'w', index=False)\n",
    "\n",
    "test_features_1.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\test_features_1.csv',mode = 'w', index=False)\n",
    "test_features_2.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\test_features_2.csv',mode = 'w', index=False)\n",
    "test_features_3.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\test_features_3.csv',mode = 'w', index=False)\n",
    "test_features_4.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\test_features_4.csv',mode = 'w', index=False)\n",
    "test_features_5.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\test_features_5.csv',mode = 'w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_1 = rf1.predict(test_features_1)\n",
    "prediction_2 = rf2.predict(test_features_2)\n",
    "prediction_3 = rf3.predict(test_features_3)\n",
    "prediction_4 = rf4.predict(test_features_4)\n",
    "prediction_5 = rf5.predict(test_features_5)\n",
    "prediction_6 = rf6.predict(test_features_6)\n",
    "prediction_7 = rf7.predict(test_features_7)\n",
    "prediction_8 = rf8.predict(test_features_8)\n",
    "prediction_9 = rf9.predict(test_features_9)\n",
    "prediction_10 = rf10.predict(test_features_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = test_features_1\n",
    "result_2 = test_features_2\n",
    "result_3 = test_features_3\n",
    "result_4 = test_features_4\n",
    "result_5 = test_features_5\n",
    "result_6 = test_features_6\n",
    "result_7 = test_features_7\n",
    "result_8 = test_features_8\n",
    "result_9 = test_features_9\n",
    "result_10 = test_features_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1['actual_fare'] = test_labels_1[['fare_amount']]\n",
    "result_2['actual_fare'] = test_labels_2[['fare_amount']]\n",
    "result_3['actual_fare'] = test_labels_3[['fare_amount']]\n",
    "result_4['actual_fare'] = test_labels_4[['fare_amount']]\n",
    "result_5['actual_fare'] = test_labels_5[['fare_amount']]\n",
    "result_6['actual_fare'] = test_labels_6[['fare_amount']]\n",
    "result_7['actual_fare'] = test_labels_7[['fare_amount']]\n",
    "result_8['actual_fare'] = test_labels_8[['fare_amount']]\n",
    "result_9['actual_fare'] = test_labels_9[['fare_amount']]\n",
    "result_10['actual_fare'] = test_labels_10[['fare_amount']]\n",
    "\n",
    "\n",
    "result_1['predicted_fare'] = prediction_1\n",
    "result_2['predicted_fare'] = prediction_2\n",
    "result_3['predicted_fare'] = prediction_3\n",
    "result_4['predicted_fare'] = prediction_4\n",
    "result_5['predicted_fare'] = prediction_5\n",
    "result_6['predicted_fare'] = prediction_6\n",
    "result_7['predicted_fare'] = prediction_7\n",
    "result_8['predicted_fare'] = prediction_8\n",
    "result_9['predicted_fare'] = prediction_9\n",
    "result_10['predicted_fare'] = prediction_10\n",
    "\n",
    "result_1['difference'] = abs(result_1['predicted_fare']-result_1['actual_fare'])\n",
    "result_2['difference'] = abs(result_2['predicted_fare']-result_2['actual_fare'])\n",
    "result_3['difference'] = abs(result_3['predicted_fare']-result_3['actual_fare'])\n",
    "result_4['difference'] = abs(result_4['predicted_fare']-result_4['actual_fare'])\n",
    "result_5['difference'] = abs(result_5['predicted_fare']-result_5['actual_fare'])\n",
    "result_6['difference'] = abs(result_6['predicted_fare']-result_6['actual_fare'])\n",
    "result_7['difference'] = abs(result_7['predicted_fare']-result_7['actual_fare'])\n",
    "result_8['difference'] = abs(result_8['predicted_fare']-result_8['actual_fare'])\n",
    "result_9['difference'] = abs(result_9['predicted_fare']-result_9['actual_fare'])\n",
    "result_10['difference'] = abs(result_10['predicted_fare']-result_10['actual_fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the results to csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_1.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\result_1.csv',mode = 'w', index=False)\n",
    "result_2.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\result_2.csv',mode = 'w', index=False)\n",
    "result_3.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\result_3.csv',mode = 'w', index=False)\n",
    "result_4.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\result_4.csv',mode = 'w', index=False)\n",
    "result_5.to_csv(r'C:\\Users\\nishi\\Desktop\\EECS 731 - Data Science\\Project\\result_5.csv',mode = 'w', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_1.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_3.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_4.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model_1 = {'month','day','hour','PULocationID','DOLocationID','fare_amount'}  \n",
    "Model_2 = {'month','day','hour','PULocationID','DOLocationID','fare_amount','airport'}  \n",
    "Model_3 = {'month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_distance'}  \n",
    "Model_4 = {'month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_duration'}  \n",
    "Model_5 = {'month','day','hour','PULocationID','DOLocationID','fare_amount','airport','trip_distance','trip_duration'} \n",
    "Model_6 = {'trip_distance','trip_duration','fare_amount'}\n",
    "Model_7 = {'day','hour','PULocationID','DOLocationID','fare_amount'}\n",
    "Model_8 = {'day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','fare_amount'}\n",
    "Model_9 = {'day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','RatecodeID','fare_amount'}\n",
    "Model_10 = {'day','hour','PULocationID','DOLocationID','weekend_Trip','trip_type','payment_type','RatecodeID','fare_amount','passenger_count'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1 Accuracy % : 13.42\n",
      "Model_2 Accuracy % : 13.75\n",
      "Model_3 Accuracy % : 61.81\n",
      "Model_4 Accuracy % : 62.80\n",
      "Model_5 Accuracy % : 93.48\n",
      "Model_6 Accuracy % : 93.46\n",
      "Model_7 Accuracy % : 13.43\n",
      "Model_8 Accuracy % : 13.37\n",
      "Model_9 Accuracy % : 13.71\n",
      "Model_10 Accuracy % : 13.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Model_1 Accuracy % : \" +str((result_1[result_1['difference'] < 1].shape[0]/result_1.shape[0]) * 100)[:5])\n",
    "print(\"Model_2 Accuracy % : \" +str((result_2[result_2['difference'] < 1].shape[0]/result_2.shape[0]) * 100)[:5])\n",
    "print(\"Model_3 Accuracy % : \" +str((result_3[result_3['difference'] < 1].shape[0]/result_3.shape[0]) * 100)[:5])\n",
    "print(\"Model_4 Accuracy % : \" +str((result_4[result_4['difference'] < 1].shape[0]/result_4.shape[0]) * 100)[:5])\n",
    "print(\"Model_5 Accuracy % : \" +str((result_5[result_5['difference'] < 1].shape[0]/result_5.shape[0]) * 100)[:5])\n",
    "print(\"Model_6 Accuracy % : \" +str((result_6[result_6['difference'] < 1].shape[0]/result_6.shape[0]) * 100)[:5])\n",
    "print(\"Model_7 Accuracy % : \" +str((result_7[result_7['difference'] < 1].shape[0]/result_7.shape[0]) * 100)[:5])\n",
    "print(\"Model_8 Accuracy % : \" +str((result_8[result_8['difference'] < 1].shape[0]/result_8.shape[0]) * 100)[:5])\n",
    "print(\"Model_9 Accuracy % : \" +str((result_9[result_9['difference'] < 1].shape[0]/result_9.shape[0]) * 100)[:5])\n",
    "print(\"Model_10 Accuracy % : \" +str((result_10[result_10['difference'] < 1].shape[0]/result_10.shape[0]) * 100)[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
